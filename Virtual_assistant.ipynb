{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Collection and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1: Read and load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = 'dataset/20000-Utterances-Training-dataset.csv' \n",
    "df = pd.read_csv(dataset_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2: Explore and analyze the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>flags</th>\n",
       "      <th>utterance</th>\n",
       "      <th>category</th>\n",
       "      <th>intent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BILC</td>\n",
       "      <td>I don't have an online account, what do I have...</td>\n",
       "      <td>ACCOUNT</td>\n",
       "      <td>create_account</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BILQZ</td>\n",
       "      <td>can you tell me if i can regisger two accounts...</td>\n",
       "      <td>ACCOUNT</td>\n",
       "      <td>create_account</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>BPLC</td>\n",
       "      <td>I have no online account, open one, please</td>\n",
       "      <td>ACCOUNT</td>\n",
       "      <td>create_account</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>BIPLD</td>\n",
       "      <td>could you ask an agent how to open an account,...</td>\n",
       "      <td>ACCOUNT</td>\n",
       "      <td>create_account</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>BLQC</td>\n",
       "      <td>i want an online account, create one</td>\n",
       "      <td>ACCOUNT</td>\n",
       "      <td>create_account</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   flags                                          utterance category  \\\n",
       "0   BILC  I don't have an online account, what do I have...  ACCOUNT   \n",
       "1  BILQZ  can you tell me if i can regisger two accounts...  ACCOUNT   \n",
       "2   BPLC         I have no online account, open one, please  ACCOUNT   \n",
       "3  BIPLD  could you ask an agent how to open an account,...  ACCOUNT   \n",
       "4   BLQC               i want an online account, create one  ACCOUNT   \n",
       "\n",
       "           intent  \n",
       "0  create_account  \n",
       "1  create_account  \n",
       "2  create_account  \n",
       "3  create_account  \n",
       "4  create_account  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 21534 entries, 0 to 21533\n",
      "Data columns (total 4 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   flags      21534 non-null  object\n",
      " 1   utterance  21534 non-null  object\n",
      " 2   category   21534 non-null  object\n",
      " 3   intent     21534 non-null  object\n",
      "dtypes: object(4)\n",
      "memory usage: 673.1+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3: Handle missing data and clean the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to clean the text by removing noise\n",
    "def clean_text(text):\n",
    "    # Remove any non-alphanumeric characters and convert to lowercase\n",
    "    text = re.sub(r'[^a-zA-Z0-9\\s]', '', text).lower()\n",
    "    # Remove extra spaces and newlines\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the clean_text function to the 'utterance' column\n",
    "df['utterance'] = df['utterance'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4: Preprocess the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Tokenization\n",
    "nltk.download('punkt')  # Download the punkt tokenizer\n",
    "df['tokenized_utterance'] = df['utterance'].apply(nltk.word_tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Stopword removal\n",
    "nltk.download('stopwords')  # Download the stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "df['filtered_utterance'] = df['tokenized_utterance'].apply(lambda tokens: [token for token in tokens if token not in stop_words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stemming or Lemmatization (choose one)\n",
    "stemmer = PorterStemmer()\n",
    "# lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['processed_utterance'] = df['filtered_utterance'].apply(lambda tokens: [stemmer.stem(token) for token in tokens])\n",
    "# df['processed_utterance'] = df['filtered_utterance'].apply(lambda tokens: [lemmatizer.lemmatize(token) for token in tokens])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 5: Encode linguistic flags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a mapping for linguistic flags to numerical values\n",
    "linguistic_flags_mapping = {\n",
    "    'B': 1,\n",
    "    'S': 2,\n",
    "    'L': 3,\n",
    "    'M': 4,\n",
    "    'I': 5,\n",
    "    'C': 6,\n",
    "    'P': 7,\n",
    "    'Q': 8,\n",
    "    'W': 9,\n",
    "    'E': 10,\n",
    "    'D': 11,\n",
    "    'Z': 12\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to encode linguistic flags\n",
    "def encode_linguistic_flags(flags):\n",
    "    return [linguistic_flags_mapping[flag] for flag in flags]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the flags before encoding\n",
    "df['flags'] = df['flags'].apply(list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the encode_linguistic_flags function to the 'flags' column\n",
    "df['encoded_flags'] = df['flags'].apply(encode_linguistic_flags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>flags</th>\n",
       "      <th>utterance</th>\n",
       "      <th>category</th>\n",
       "      <th>intent</th>\n",
       "      <th>tokenized_utterance</th>\n",
       "      <th>filtered_utterance</th>\n",
       "      <th>processed_utterance</th>\n",
       "      <th>encoded_flags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[B, I, L, C]</td>\n",
       "      <td>i dont have an online account what do i have t...</td>\n",
       "      <td>ACCOUNT</td>\n",
       "      <td>create_account</td>\n",
       "      <td>[i, dont, have, an, online, account, what, do,...</td>\n",
       "      <td>[dont, online, account, register]</td>\n",
       "      <td>[dont, onlin, account, regist]</td>\n",
       "      <td>[1, 5, 3, 6]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[B, I, L, Q, Z]</td>\n",
       "      <td>can you tell me if i can regisger two accounts...</td>\n",
       "      <td>ACCOUNT</td>\n",
       "      <td>create_account</td>\n",
       "      <td>[can, you, tell, me, if, i, can, regisger, two...</td>\n",
       "      <td>[tell, regisger, two, accounts, single, email,...</td>\n",
       "      <td>[tell, regisg, two, account, singl, email, add...</td>\n",
       "      <td>[1, 5, 3, 8, 12]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[B, P, L, C]</td>\n",
       "      <td>i have no online account open one please</td>\n",
       "      <td>ACCOUNT</td>\n",
       "      <td>create_account</td>\n",
       "      <td>[i, have, no, online, account, open, one, please]</td>\n",
       "      <td>[online, account, open, one, please]</td>\n",
       "      <td>[onlin, account, open, one, pleas]</td>\n",
       "      <td>[1, 7, 3, 6]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[B, I, P, L, D]</td>\n",
       "      <td>could you ask an agent how to open an account ...</td>\n",
       "      <td>ACCOUNT</td>\n",
       "      <td>create_account</td>\n",
       "      <td>[could, you, ask, an, agent, how, to, open, an...</td>\n",
       "      <td>[could, ask, agent, open, account, please]</td>\n",
       "      <td>[could, ask, agent, open, account, pleas]</td>\n",
       "      <td>[1, 5, 7, 3, 11]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[B, L, Q, C]</td>\n",
       "      <td>i want an online account create one</td>\n",
       "      <td>ACCOUNT</td>\n",
       "      <td>create_account</td>\n",
       "      <td>[i, want, an, online, account, create, one]</td>\n",
       "      <td>[want, online, account, create, one]</td>\n",
       "      <td>[want, onlin, account, creat, one]</td>\n",
       "      <td>[1, 3, 8, 6]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             flags                                          utterance  \\\n",
       "0     [B, I, L, C]  i dont have an online account what do i have t...   \n",
       "1  [B, I, L, Q, Z]  can you tell me if i can regisger two accounts...   \n",
       "2     [B, P, L, C]           i have no online account open one please   \n",
       "3  [B, I, P, L, D]  could you ask an agent how to open an account ...   \n",
       "4     [B, L, Q, C]                i want an online account create one   \n",
       "\n",
       "  category          intent                                tokenized_utterance  \\\n",
       "0  ACCOUNT  create_account  [i, dont, have, an, online, account, what, do,...   \n",
       "1  ACCOUNT  create_account  [can, you, tell, me, if, i, can, regisger, two...   \n",
       "2  ACCOUNT  create_account  [i, have, no, online, account, open, one, please]   \n",
       "3  ACCOUNT  create_account  [could, you, ask, an, agent, how, to, open, an...   \n",
       "4  ACCOUNT  create_account        [i, want, an, online, account, create, one]   \n",
       "\n",
       "                                  filtered_utterance  \\\n",
       "0                  [dont, online, account, register]   \n",
       "1  [tell, regisger, two, accounts, single, email,...   \n",
       "2               [online, account, open, one, please]   \n",
       "3         [could, ask, agent, open, account, please]   \n",
       "4               [want, online, account, create, one]   \n",
       "\n",
       "                                 processed_utterance     encoded_flags  \n",
       "0                     [dont, onlin, account, regist]      [1, 5, 3, 6]  \n",
       "1  [tell, regisg, two, account, singl, email, add...  [1, 5, 3, 8, 12]  \n",
       "2                 [onlin, account, open, one, pleas]      [1, 7, 3, 6]  \n",
       "3          [could, ask, agent, open, account, pleas]  [1, 5, 7, 3, 11]  \n",
       "4                 [want, onlin, account, creat, one]      [1, 3, 8, 6]  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print the preprocessed dataset\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intent Recognition Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1: Split the dataset into training, validation, and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df['processed_utterance']  # Features (processed utterances)\n",
    "y = df['intent']  # Labels (intents)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2: Convert processed utterances to numerical vectors using TF-IDF vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer_intent = TfidfVectorizer()\n",
    "X_train_tfidf = tfidf_vectorizer_intent.fit_transform(X_train.apply(lambda tokens: ' '.join(tokens)))\n",
    "X_test_tfidf = tfidf_vectorizer_intent.transform(X_test.apply(lambda tokens: ' '.join(tokens)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3: Define the hyperparameter grid for SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_dist = {\n",
    "    'C': np.logspace(-3, 3, 7),  # Regularization parameter\n",
    "    'kernel': ['linear', 'rbf'],  # Kernel type\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4: Initialize the SVM classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_classifier = SVC()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 5: Hyperparameter Optimization using RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomizedSearchCV(cv=3, estimator=SVC(),\n",
       "                   param_distributions={&#x27;C&#x27;: array([1.e-03, 1.e-02, 1.e-01, 1.e+00, 1.e+01, 1.e+02, 1.e+03]),\n",
       "                                        &#x27;kernel&#x27;: [&#x27;linear&#x27;, &#x27;rbf&#x27;]},\n",
       "                   random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomizedSearchCV</label><div class=\"sk-toggleable__content\"><pre>RandomizedSearchCV(cv=3, estimator=SVC(),\n",
       "                   param_distributions={&#x27;C&#x27;: array([1.e-03, 1.e-02, 1.e-01, 1.e+00, 1.e+01, 1.e+02, 1.e+03]),\n",
       "                                        &#x27;kernel&#x27;: [&#x27;linear&#x27;, &#x27;rbf&#x27;]},\n",
       "                   random_state=42)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: SVC</label><div class=\"sk-toggleable__content\"><pre>SVC()</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SVC</label><div class=\"sk-toggleable__content\"><pre>SVC()</pre></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "RandomizedSearchCV(cv=3, estimator=SVC(),\n",
       "                   param_distributions={'C': array([1.e-03, 1.e-02, 1.e-01, 1.e+00, 1.e+01, 1.e+02, 1.e+03]),\n",
       "                                        'kernel': ['linear', 'rbf']},\n",
       "                   random_state=42)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_search = RandomizedSearchCV(svm_classifier, param_distributions=param_dist, n_iter=10, cv=3, random_state=42)\n",
    "random_search.fit(X_train_tfidf, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 6: Get the best hyperparameters and the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = random_search.best_params_\n",
    "best_model = random_search.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 7: Evaluate the best model on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = best_model.predict(X_test_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'kernel': 'linear', 'C': 10.0}\n",
      "Best Model Accuracy: 0.9881588112375204\n"
     ]
    }
   ],
   "source": [
    "# Calculate accuracy and other relevant metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Best Hyperparameters:\", best_params)\n",
    "print(\"Best Model Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "                          precision    recall  f1-score   support\n",
      "\n",
      "            cancel_order       1.00      1.00      1.00        10\n",
      "            change_order       0.99      0.98      0.99       183\n",
      " change_shipping_address       1.00      1.00      1.00        25\n",
      "  check_cancellation_fee       1.00      1.00      1.00        66\n",
      "          check_invoices       0.90      0.98      0.94       208\n",
      "   check_payment_methods       0.98      0.96      0.97        52\n",
      "     check_refund_policy       1.00      0.98      0.99        92\n",
      "               complaint       1.00      1.00      1.00       146\n",
      "contact_customer_service       1.00      1.00      1.00       430\n",
      "     contact_human_agent       1.00      1.00      1.00       198\n",
      "          create_account       0.99      1.00      0.99       367\n",
      "          delete_account       0.99      0.98      0.98       204\n",
      "        delivery_options       0.97      0.97      0.97        74\n",
      "         delivery_period       1.00      0.97      0.98        32\n",
      "            edit_account       1.00      0.96      0.98        25\n",
      "             get_invoice       0.98      0.91      0.95       269\n",
      "              get_refund       0.99      0.99      0.99       221\n",
      " newsletter_subscription       1.00      1.00      1.00        49\n",
      "           payment_issue       1.00      1.00      1.00       894\n",
      "             place_order       1.00      1.00      1.00        14\n",
      "        recover_password       1.00      1.00      1.00       215\n",
      "   registration_problems       1.00      1.00      1.00        27\n",
      "                  review       1.00      1.00      1.00       101\n",
      " set_up_shipping_address       1.00      1.00      1.00        25\n",
      "          switch_account       1.00      1.00      1.00        66\n",
      "             track_order       0.96      1.00      0.98       252\n",
      "            track_refund       1.00      0.94      0.97        62\n",
      "\n",
      "                accuracy                           0.99      4307\n",
      "               macro avg       0.99      0.99      0.99      4307\n",
      "            weighted avg       0.99      0.99      0.99      4307\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Generate a classification report for more detailed evaluation\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 8: Save the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['models/tfidf_vectorizer_intent.pkl']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "# Save the trained model to a file \n",
    "joblib.dump(best_model, \"models/intent_recognition_model.pkl\")\n",
    "\n",
    "# Save the TF-IDF vectorizer to a file\n",
    "joblib.dump(tfidf_vectorizer_intent, \"models/tfidf_vectorizer_intent.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Category Classification Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1: Split the dataset into training, validation, and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df['processed_utterance']  # Features (processed utterances)\n",
    "y_category = df['category']  # Labels (high-level intent categories)\n",
    "X_train, X_test, y_train_category, y_test_category = train_test_split(X, y_category, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2: Convert processed utterances to numerical vectors using TF-IDF vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer_category = TfidfVectorizer()\n",
    "X_train_tfidf = tfidf_vectorizer_category.fit_transform(X_train.apply(lambda tokens: ' '.join(tokens)))\n",
    "X_test_tfidf = tfidf_vectorizer_category.transform(X_test.apply(lambda tokens: ' '.join(tokens)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3: Define the hyperparameter grid for SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_dist = {\n",
    "    'C': np.logspace(-3, 3, 7),  # Regularization parameter\n",
    "    'kernel': ['linear', 'rbf'],  # Kernel type\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4: Initialize the SVM classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_classifier_category = SVC()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 5: Hyperparameter Optimization using RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomizedSearchCV(cv=3, estimator=SVC(),\n",
       "                   param_distributions={&#x27;C&#x27;: array([1.e-03, 1.e-02, 1.e-01, 1.e+00, 1.e+01, 1.e+02, 1.e+03]),\n",
       "                                        &#x27;kernel&#x27;: [&#x27;linear&#x27;, &#x27;rbf&#x27;]},\n",
       "                   random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" ><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomizedSearchCV</label><div class=\"sk-toggleable__content\"><pre>RandomizedSearchCV(cv=3, estimator=SVC(),\n",
       "                   param_distributions={&#x27;C&#x27;: array([1.e-03, 1.e-02, 1.e-01, 1.e+00, 1.e+01, 1.e+02, 1.e+03]),\n",
       "                                        &#x27;kernel&#x27;: [&#x27;linear&#x27;, &#x27;rbf&#x27;]},\n",
       "                   random_state=42)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" ><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: SVC</label><div class=\"sk-toggleable__content\"><pre>SVC()</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-6\" type=\"checkbox\" ><label for=\"sk-estimator-id-6\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SVC</label><div class=\"sk-toggleable__content\"><pre>SVC()</pre></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "RandomizedSearchCV(cv=3, estimator=SVC(),\n",
       "                   param_distributions={'C': array([1.e-03, 1.e-02, 1.e-01, 1.e+00, 1.e+01, 1.e+02, 1.e+03]),\n",
       "                                        'kernel': ['linear', 'rbf']},\n",
       "                   random_state=42)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_search_category = RandomizedSearchCV(svm_classifier_category, param_distributions=param_dist, n_iter=10, cv=3, random_state=42)\n",
    "random_search_category.fit(X_train_tfidf, y_train_category)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 6: Get the best hyperparameters and the best model for category classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params_category = random_search_category.best_params_\n",
    "best_model_category = random_search_category.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 7: Evaluate the best model on the test set for category classification (Using best_model_category for prediction)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_category = best_model_category.predict(X_test_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters for Category Classification: {'kernel': 'linear', 'C': 1000.0}\n",
      "Category Classification Accuracy: 0.9960529370791734\n"
     ]
    }
   ],
   "source": [
    "# Calculate accuracy and other relevant metrics for category classification\n",
    "accuracy_category = accuracy_score(y_test_category, y_pred_category)\n",
    "print(\"Best Hyperparameters for Category Classification:\", best_params_category)\n",
    "print(\"Category Classification Accuracy:\", accuracy_category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report for Category Classification:\n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "         ACCOUNT       1.00      1.00      1.00       904\n",
      "CANCELLATION_FEE       1.00      0.98      0.99        66\n",
      "         CONTACT       1.00      1.00      1.00       628\n",
      "        DELIVERY       0.99      0.96      0.98       106\n",
      "        FEEDBACK       1.00      1.00      1.00       247\n",
      "        INVOICES       1.00      1.00      1.00       477\n",
      "      NEWSLETTER       1.00      1.00      1.00        49\n",
      "           ORDER       0.98      1.00      0.99       459\n",
      "         PAYMENT       1.00      1.00      1.00       946\n",
      "         REFUNDS       0.99      0.99      0.99       375\n",
      "        SHIPPING       1.00      0.98      0.99        50\n",
      "\n",
      "        accuracy                           1.00      4307\n",
      "       macro avg       1.00      0.99      0.99      4307\n",
      "    weighted avg       1.00      1.00      1.00      4307\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Generate a classification report for more detailed evaluation for category classification\n",
    "print(\"Classification Report for Category Classification:\")\n",
    "print(classification_report(y_test_category, y_pred_category))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 8: Save the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['models/tfidf_vectorizer_category.pkl']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "# Save the trained model to a file\n",
    "joblib.dump(best_model_category, \"models/category_classification_model.pkl\")\n",
    "\n",
    "# Save the TF-IDF vectorizer to a file\n",
    "joblib.dump(tfidf_vectorizer_category, \"models/tfidf_vectorizer_category.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Integration with Virtual Assistant Framework"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1: Load the trained models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the trained Intent Recognition model from the file\n",
    "intent_recognition_model = joblib.load(\"models/intent_recognition_model.pkl\")\n",
    "\n",
    "# Load the TF-IDF vectorizer for Intent Recognition from the file\n",
    "tfidf_vectorizer_intent = joblib.load(\"models/tfidf_vectorizer_intent.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the intent responses for each intent category\n",
    "intent_responses = {\n",
    "    \"cancel_order\": \"Your order has been canceled successfully.\",\n",
    "    \"change_order\": \"Your order has been updated.\",\n",
    "    \"change_shipping_address\": \"Your shipping address has been changed.\",\n",
    "    \"check_cancellation_fee\": \"The cancellation fee for your order is $10.\",\n",
    "    \"check_invoices\": \"You can view and download your invoices from your account dashboard.\",\n",
    "    \"check_payment_methods\": \"We accept various payment methods, including credit cards, debit cards, and PayPal.\",\n",
    "    \"check_refund_policy\": \"Our refund policy allows for full refunds within 30 days of purchase.\",\n",
    "    \"complaint\": \"I'm sorry to hear that you're facing an issue. Please provide more details, and we'll assist you.\",\n",
    "    \"contact_customer_service\": \"You can reach our customer service team at [Phone Number] or [Email Address].\",\n",
    "    \"contact_human_agent\": \"One of our human agents will be happy to assist you shortly.\",\n",
    "    \"create_account\": \"You can create an online account on our website. Please visit the 'Create Account' page to get started.\",\n",
    "    \"delete_account\": \"Your account has been deleted successfully.\",\n",
    "    \"delivery_options\": \"You can choose from standard or express shipping options during checkout.\",\n",
    "    \"delivery_period\": \"The estimated delivery time for your order is 2-3 business days.\",\n",
    "    \"edit_account\": \"You can edit your account information on the 'Account Settings' page.\",\n",
    "    \"get_invoice\": \"You can view and download your invoice for the latest order in your account dashboard.\",\n",
    "    \"get_refund\": \"Your refund request has been processed, and the amount will be credited back to your original payment method.\",\n",
    "    \"newsletter_subscription\": \"Thank you for subscribing to our newsletter!\",\n",
    "    \"payment_issue\": \"Please provide more details about the payment issue, and we'll assist you.\",\n",
    "    \"place_order\": \"Your order has been successfully placed.\",\n",
    "    \"recover_password\": \"You can reset your password by clicking on the 'Forgot Password' link on the login page.\",\n",
    "    \"registration_problems\": \"I'm sorry to hear that you're experiencing registration issues. Please provide more details, and we'll assist you.\",\n",
    "    \"review\": \"Thank you for your review! Your feedback is valuable to us.\",\n",
    "    \"set_up_shipping_address\": \"You can add or update your shipping address in your account settings.\",\n",
    "    \"switch_account\": \"You can switch to a different account by logging out and then logging in with the desired account credentials.\",\n",
    "    \"track_order\": \"You can track your order using the tracking number provided in the order confirmation email.\",\n",
    "    \"track_refund\": \"Your refund request is being processed, and you will receive a confirmation email once it's completed.\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Convert text to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove punctuation\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    \n",
    "    # Remove numbers\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    \n",
    "    # Tokenize the text\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "    \n",
    "    # Apply stemming or lemmatization (if needed)\n",
    "    stemmer = PorterStemmer()\n",
    "    tokens = [stemmer.stem(token) for token in tokens]\n",
    "    \n",
    "    return tokens\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2: Create a function to handle user inputs and predict the intent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_intent(user_input):\n",
    "    processed_input = preprocess_text(user_input)\n",
    "    input_tfidf = tfidf_vectorizer_intent.transform([' '.join(processed_input)])\n",
    "    predicted_intent = intent_recognition_model.predict(input_tfidf)[0]\n",
    "    return predicted_intent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3: Create a function to predict the category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_category(user_input):\n",
    "    processed_input = preprocess_text(user_input)\n",
    "    input_tfidf = tfidf_vectorizer_category.transform([' '.join(processed_input)])\n",
    "    predicted_category = category_classification_model.predict(input_tfidf)[0]\n",
    "    return predicted_category"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4: Implement the conversation flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def virtual_assistant():\n",
    "    print(\"Virtual Assistant: Hi! How can I assist you today?\")\n",
    "    \n",
    "    while True:\n",
    "        user_input = input(\"You: \")\n",
    "        \n",
    "        if user_input.lower() == \"exit\":\n",
    "            print(\"Virtual Assistant: Goodbye!\")\n",
    "            break\n",
    "        \n",
    "        # Predict the user intent using the intent recognition model\n",
    "        predicted_intent = predict_intent(user_input)\n",
    "        \n",
    "        # Predict the category for the user input using the category classification model\n",
    "        predicted_category = predict_category(user_input)\n",
    "        \n",
    "        # Retrieve the response for the predicted intent based on the intent_responses dictionary\n",
    "        response = intent_responses.get(predicted_intent, \"I'm sorry, I don't understand.\")\n",
    "        \n",
    "        # Print the response\n",
    "        print(\"Virtual Assistant:\", response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 5: Run the virtual assistant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "virtual_assistant()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
